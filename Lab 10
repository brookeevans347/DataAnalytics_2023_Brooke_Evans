#Brooke Evans Lab 10 4000 Level

library(e1071)
set.seed(1)
#we now use the svm() function to fit the support vector classifier for a given value of the cost parameter.
#here we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decison boundary.
#We begin by generating the observations, which belong to two classes.
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
x
y
#we begin bu checking where the classes are linearly separable.
plot(x, col=3-y))
#they are not. next we fit the support vector classifier.
#note that in order for the svm() function to perform classification, we must encode the response as a factor variable.
#we now create a data frame with the response coded as a factor.

dat <- data.frame(x = x,y = as.factor(y))
svmfit <- svm(y~., data = dat, kernal = "linear", cost = 10, scale = FALSE)

#the arguement scale=FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one; depending on the application, one might prefer to use scale=true

#we can now plot the support vector classifier obtained.
plot(svmfit, dat)

#note that the two arguments to the plot.svm() are the output of the call to svm(), as well as the data used in the call to svm()
#the region of feature space that will be assigned to the -1 class is shown in light blue, and the region that will be assigned to the +1 class is shown in purple.

#we can determine the identities of those support vectors by
svmfit$index
#this will spit out the numbers that are the support vectors.

#we can obtain some basic information aout the support vector classifier fit using the summary() command:
summary(svmfit)
#this tells us, for instance, that a linear kernel was used with cost=10, and that there were seven support vectors, four in one class and three in the other (may change depending on seed and cost).

#now cost = 0.1
svmfit <- svm(y~., data=dat, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit,dat)
svmfit$index

#now tuning the svm cost functions
set.seed(1)
tune.out <- tune(svm, y ~., data=dat, kernel="linear", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
#we can easily access the cross-validation errors for each of these models using the summary() command
summary(tune.out)
#we see that cost = 0.1 results in the lowest cross-validation error rate.
#the tune() function stores the best model obtained, which can be accessed as follows:
bestmod=tune.out$best.model
summary(bestmod)
#the predict() function can be used to predict the class label on a set of test observations at any given value of the cost parameter. we begin by generating a test data set.
xtest=matrix(rnorm(20*2), ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))
#now we predict the class labeds of these test observations
#here we use the best model obtained through cross-validaton in order to make preditions.
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)

#thus, with this value of cose, 19 of the test observations were correctly classified.
#what if we had instead used cost-0.1?
svmfit <- svm (y~., data=dat, kernal="linear", cost=0.1, scale=false
ypred=predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)

#in this case one additional observation is misclassified.
#now consider a situation where the two classes are linearly seperable.
#then we can find a separating hyperplane using the svm() function.
#we first further separate the two classes in our simulated data so they are linearly separable.
x[y==1,]=x[y==1,]+0.5
plot(x, col(y+5)/2, pch=19)

#now the observations are just barely linearly separable.
#we fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified.
dat=data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit,dat)

#no training errors were made and only three support vectors were used.
#however we can see from the figure that the margin is very narrow (because the observations that are not support vectors indicated as circles are very close to the decision boundary). it seems likely that this model will perform poorly on test data.
#we now try a similiar value of cost:
svmfit <- svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,dat)

#using cost=1, we misclassify a training  observation, but we also obtain a much wider margin and make use of seven support vectors.
#it seems likely that this model will preform better on test data than the model with cost=1e5.

#----------
#New data set
#we now examine the Khan data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors.
#for each tissue sample, gene expression measurements are available.
#the data set consists of training data, xtrain and ytrain, and testing data, xtest and ytest.

library(e1071)
library(ISLR)
names(Khan)

#let's examine the dimensions of the data:
#this data set consists of expression measurements for 2308 genes, and the training and test sets consist of 63 and 20 observations, respectively.
dim(Khan$xtrain)
dim(Khan$xtest)

length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)

#we will use a support vector approach to predict cancer subtype using gene expression measurements. 
#in this data set, there are a very large number of features relative to the number of observations
#results from using a polynomial or radial kernel is unnecessary.
dat <- data.fram(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out <- svm(y~.,data=dat, kernel="linear", cost=10)
summary(out)

#we see that there are no training errors. in fact, this is not surprising because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes. 
#we are most interested not in the support vector classifier's performance on the training observations, but on the test observations.

dat.te=dataframe(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out,newdata=dat.te)
#stopped here.
